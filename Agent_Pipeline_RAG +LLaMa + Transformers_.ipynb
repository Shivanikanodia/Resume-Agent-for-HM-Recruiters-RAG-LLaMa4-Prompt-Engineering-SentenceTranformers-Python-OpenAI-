{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf9e0a44-54b6-4010-89fd-8e3c3a08c5b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####  Resume Summarization AI Agent (RAG + LLM):\n",
    "An end-to-end pipeline that uses Retrieval-Augmented Generation (RAG), Sentence Transformers, Re-ranking and an LLM to generate concise, role-aware candidate resume summaries.\n",
    "\n",
    "####  Problem:\n",
    "Recruiters and hiring managers often review hundreds of resumes manually—slow, inconsistent, and error-prone. This agent extracts the information that actually matters to hiring teams and produces fast, consistent summaries.\n",
    "\n",
    "####  Goal:\n",
    "Serve Hiring Managers and Recruiters across functions (Engineering, Global Functions, Professional Services, etc.). Let users query large resume sets by skills, experience, and team fit. Reduce time-to-screen by surfacing the most relevant candidates and summaries.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3b7a798-a881-434f-8cbf-fb3d9e312f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## How It Works (Architecture):\n",
    "\n",
    " - Data Loading: Read resumes stored in Unity Catalog Volumes / Delta.\n",
    "\n",
    "- Chunking & Embeddings: Split resumes into ~800-character chunks.\n",
    "\n",
    "- Embed with all-MiniLM-L6-v2 (384-dim) via SentenceTransformer.\n",
    "\n",
    "- Semantic Retrieval (Recall): Cosine similarity over embeddings to fetch top-k relevant chunks (FAISS or equivalent index recommended).\n",
    "\n",
    "- Re-ranking (Precision): Cross-encoder scores (query, chunk) pairs and reorders the retrieved set to keep the most relevant passages.\n",
    "\n",
    "- Prompt Construction: Insert top chunks into a concise, instruction-driven prompt with rules/constraints (focus on role fit, impact, skills, recency).\n",
    "\n",
    "- LLM Generation: Call Databricks-hosted Llama endpoint to produce the final summary/answer, REST API Calls (via requests) – To communicate with the Databricks LLM endpoint for inference \n",
    "\n",
    "- Evaluation & Observability: Track latency, error rate, retrieval quality, and token/cost.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e194dde-26bc-4595-93ea-304433ba183c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\nCollecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\nCollecting tqdm (from sentence-transformers)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nCollecting torch>=1.11.0 (from sentence-transformers)\n  Downloading torch-2.9.0-cp311-cp311-manylinux_2_28_aarch64.whl.metadata (30 kB)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.11/site-packages (from sentence-transformers) (1.11.1)\nCollecting huggingface-hub>=0.20.0 (from sentence-transformers)\n  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: Pillow in /databricks/python3/lib/python3.11/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /databricks/python3/lib/python3.11/site-packages (from sentence-transformers) (4.10.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.4)\nCollecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\nCollecting httpx<1,>=0.23.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\nCollecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting typer-slim (from huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\nCollecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_28_aarch64.whl.metadata (4.9 kB)\nCollecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\nCollecting jinja2 (from torch>=1.11.0->sentence-transformers)\n  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting huggingface-hub>=0.20.0 (from sentence-transformers)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.23.5)\nCollecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.31.0)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.8 kB)\nCollecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.1 kB)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\nCollecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nCollecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (2.7 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.7.22)\nCollecting anyio (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\nCollecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\nCollecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: click>=8.0.0 in /databricks/python3/lib/python3.11/site-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.0.4)\nDownloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\nDownloading torch-2.9.0-cp311-cp311-manylinux_2_28_aarch64.whl (104.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/104.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m34.6/104.2 MB\u001B[0m \u001B[31m191.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.9/104.2 MB\u001B[0m \u001B[31m157.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m93.6/104.2 MB\u001B[0m \u001B[31m155.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m104.1/104.2 MB\u001B[0m \u001B[31m155.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m104.2/104.2 MB\u001B[0m \u001B[31m125.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/12.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.0/12.0 MB\u001B[0m \u001B[31m144.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/566.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m566.1/566.1 kB\u001B[0m \u001B[31m20.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\nDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_28_aarch64.whl (3.2 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/3.2 MB\u001B[0m \u001B[31m81.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m92.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading regex-2025.11.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (793 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/793.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m793.4/793.4 kB\u001B[0m \u001B[31m36.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (473 kB)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/6.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m151.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m93.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\nDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (24 kB)\nDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/536.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m20.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: mpmath, tqdm, sympy, safetensors, regex, networkx, MarkupSafe, hf-xet, fsspec, jinja2, huggingface-hub, torch, tokenizers, transformers, sentence-transformers\nSuccessfully installed MarkupSafe-3.0.3 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 regex-2025.11.3 safetensors-0.6.2 sentence-transformers-5.1.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.0 tqdm-4.67.1 transformers-4.57.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# resume_summary_pipeline.py\n",
    "\n",
    "%pip install sentence-transformers\n",
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74659dd-be47-4831-8a98-dd45d1f9d4fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1. Reading and Chunking Resume: \n",
    "\n",
    "LLMs perform better with concise inputs. We are chunking long resumes into small parts (~800 chars) to be later searched semantically using **read_chunk_resume** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2b685c-3f53-470a-a548-34472330d061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "def read_and_chunk_resume(path, width=800):\n",
    "    text = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return textwrap.wrap(text, width=width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "233435e8-74db-4872-bd5f-317876d31e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Path(path).read_text(),** Reads the content of the file at the specified path. \n",
    "\n",
    "**textwrap.wrap(text, width=800),** Splits the long resume string into chunks of **~800 characters** and which avoids cutting off, which improves token management for LLMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0568c086-c88f-4646-91c9-4b97a5cc5239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2.  Generating Embeddings:\n",
    "Embeddings converts chunks into high-dimensional vectors so we can find semantically similar text based on a query.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cddcf01-3c50-4dce-9de7-2191b74292f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embeddings(chunks, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, normalize_embeddings=True)\n",
    "    return embeddings, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74a6925b-269e-4c87-a580-ac121f59844c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The SentenceTransformer('all-MiniLM-L6-v2') model converts text chunks into dense numeric vectors (embeddings) that represent their meaning. The model.encode() function takes a list of text chunks and creates a 384-dimensional embedding for each one. This process returns two things:\n",
    "\n",
    "Embeddings – a list of vector representations for each text chunk.\n",
    "\n",
    "The model – which can be reused later to embed new text or queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eec0810-dc80-465f-8421-766c8528ee70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####  3.Re-ranking using Cross Encoder to score and order the relevant chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9485bc6f-8bca-4801-9750-3f1b84c8777f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank_with_cross_encoder(query, candidate_pairs, cross_encoder_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_k=3):\n",
    "    ce = CrossEncoder(cross_encoder_name)\n",
    "    pairs = [(query, txt) for _, txt in candidate_pairs]\n",
    "    scores = ce.predict(pairs)  # higher = better\n",
    "    order = np.argsort(-scores)[:top_k]\n",
    "    return [candidate_pairs[i][0] for i in order]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b110daa-83d2-450c-9004-a846d81dd479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This function reranks text chunks based on their relevance to different queries  using a pre-trained CrossEncoder model.\n",
    "It pairs the query with each text chunk, predicts relevance scores, sorts them in descending order, and returns the indices of the top_k most relevant chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d233b211-06ea-4a42-8f66-e12f89e3b741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_top_k_chunks(query, chunks, embeddings, model, top_k=5, use_cross_encoder=True, bi_encoder_candidates=10):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = cosine_similarity([q_emb], embeddings)[0]\n",
    "    cand_ids = np.argsort(-sims)[:max(top_k, bi_encoder_candidates)]\n",
    "    candidates = [(i, chunks[i]) for i in cand_ids]\n",
    "\n",
    "    if use_cross_encoder:\n",
    "        top_ids = rerank_with_cross_encoder(query, candidates, top_k=top_k)\n",
    "    else:\n",
    "        top_ids = list(cand_ids[:top_k])\n",
    "\n",
    "    return [chunks[i] for i in top_ids]  # ✅ fixed missing bracket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9c17c14-59dd-4c3c-9909-f85c7997ab64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The function first uses a bi-encoder to quickly find the most similar chunks to a query based on cosine similarity.\n",
    "Then, if enabled, it uses a cross-encoder to rerank those top candidates for higher accuracy before returning the best top_k chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efbe4d06-392c-4eaf-a82f-031edf712a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e26c0cfe-8003-453c-95a6-fca5b07fa9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Calling Databricks LLM endpoint LLAMA-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be43b20-6c5f-487c-91d3-d0ab387833c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def call_databricks_llm(prompt, model=\"databricks-llama-4-maverick\", max_tokens=500):\n",
    "    os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST\n",
    "    os.environ[\"DATABRICKS_TOKEN\"] = DATABRICKS_TOKEN\n",
    "\n",
    "    api_url = f\"{DATABRICKS_HOST}/serving-endpoints/{model}/invocations\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ['DATABRICKS_TOKEN']}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes resumes.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        r = response.json()\n",
    "        if \"predictions\" in r:\n",
    "            return r[\"predictions\"][0][\"message\"][\"content\"]\n",
    "        elif \"choices\" in r:\n",
    "            return r[\"choices\"][0][\"message\"][\"content\"]\n",
    "        elif \"data\" in r:\n",
    "            return r[\"data\"][\"message\"]\n",
    "        else:\n",
    "            raise Exception(f\"Unknown response structure: {r}\")\n",
    "    else:\n",
    "        raise Exception(f\"Request failed: {response.status_code} — {response.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ecdef29-bcfe-4f86-bba5-d8187016d70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This script interacts with a Databricks-hosted large language model (LLM) by sending a prompt and receiving a summarized response. The API URL is constructed dynamically using the model name to ensure the request reaches the correct endpoint.\n",
    "\n",
    "The payload follows the OpenAI-style chat format, which includes a system message to set the assistant's behavior and a user message containing the actual prompt. \n",
    "\n",
    "The max_tokens parameter is used to limit the length of the model's response, ensuring it remains concise and controlled. Finally, the requests.post() function is used to make the HTTP request, sending the payload and headers to the LLM endpoint and retrieving the generated output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9fc4e1b-0b86-4b41-8452-60f52ceb60b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Format Prompt with Prompt Engineering: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189cd9f2-cf22-43d3-8e76-18a8448e69e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_prompt(context, query):\n",
    "    return f\"\"\"\n",
    "[System Role]\n",
    "You are a Resume Summarization Agent for hiring managers and recruiters.\n",
    "\n",
    "You understand:\n",
    "- Resume structure (Experience, Skills, Responsibilites, Projects, Education, Certifications)\n",
    "- Industry-specific terminologies.\n",
    "- Team Fitment and Cultural Fitment. \n",
    "- Red flags and Job hopping, \n",
    "\n",
    "Goal:\n",
    "Summarize resumes concisely for fast screening.\n",
    "\n",
    "Resume:\n",
    "{context}\n",
    "\n",
    "Task:\n",
    "Summarize the candidate profile using these guidelines:\n",
    "\n",
    "1. Include Companies, domain familiarity, and experience.\n",
    "2. Highlight tools and core skills. \n",
    "3. Mention notable accomplishments/achievements and measurable impact.\n",
    "4. Include relevant responsibilities and projects. \n",
    "5. Use clear bullet points only. \n",
    "6. Identify gaps >6 months or job hopping.\n",
    "\n",
    "Constraints:\n",
    "- Avoid hallucination. \n",
    "- Use only text from resume.\n",
    "\n",
    "Query:\n",
    "**\"{query}\"**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe2c6b92-1e74-4230-afef-f40a5f718075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "_relevance_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def relevance_score(summary, context_chunks):\n",
    "    summary_emb = _relevance_model.encode([summary], normalize_embeddings=True)\n",
    "    context_emb = _relevance_model.encode([\" \".join(context_chunks)], normalize_embeddings=True)\n",
    "    return float(cosine_similarity(summary_emb, context_emb)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5bfa6fd-bfa9-40a9-acd9-a5a36915fe44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def summarize_resume(file_path_txt, query):\n",
    "    chunks = read_and_chunk_resume(file_path_txt)\n",
    "    embeddings, model = get_embeddings(chunks)\n",
    "    top_chunks = get_top_k_chunks(query, chunks, embeddings, model)\n",
    "    context = \"\\n\\n\".join(top_chunks)\n",
    "    prompt = format_prompt(context, query)\n",
    "    summary = call_databricks_llm(prompt)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c673d201-1682-43bc-a4ca-879441fa4d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This function computes how relevant a generated summary is to the original context using a semantic similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc4ccd2-fd30-4a64-b9a2-8cdb88654025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def summarize_all_resumes(file_paths, query):\n",
    "    results = []\n",
    "    errors = 0\n",
    "    total_time = 0\n",
    "    relevance_scores = []\n",
    "\n",
    "    for path in file_paths:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            result = summarize_resume(path, query)\n",
    "            result_text = result if isinstance(result, str) else result[0]\n",
    "            relevance = relevance_score(result_text, read_and_chunk_resume(path))\n",
    "            results.append((path, result_text, relevance))\n",
    "            relevance_scores.append(relevance)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error on {path}: {e}\")\n",
    "            errors += 1\n",
    "        total_time += time.time() - start\n",
    "\n",
    "    print(\"\\n\uD83D\uDCCA Evaluation Summary:\")\n",
    "    print(f\"Average latency: {total_time/len(file_paths):.2f} sec/resume\")\n",
    "    print(f\"Error rate: {errors/len(file_paths)*100:.1f}%\")\n",
    "    print(f\"Average relevance score: {np.mean(relevance_scores):.2f}\")\n",
    "\n",
    "    print(\"\\n✅ Summaries:\")\n",
    "    for path, summary, rel in results:\n",
    "        print(f\"\\n--- {path} ---\\nRelevance: {rel:.2f}\\n{summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7c9f33d-f907-4509-ad6a-2d6c6296dbbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This function loops through multiple resumes, summarizes each one using your summarize_resume() function, then measures:\n",
    "\n",
    "- Latency (how long each took)\n",
    "- Error rate, and\n",
    "- Average relevance (how semantically similar each summary is to its original resume, via your relevance_score() function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d6c19f2-5821-4366-b3bf-17473715aa0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA Evaluation Summary:\nAverage latency: 6.38 sec/resume\nError rate: 0.0%\nAverage relevance score: 0.69\n\n✅ Summaries:\n\n--- /Volumes/workspace/default/default_resume/Haiming Wang.txt ---\nRelevance: 0.74\nHere's a summary of the candidate profile based on Python and ML skills:\n\n* **Companies and Experience:** \n  + Q-lytics Consulting (Data Scientist, Remote Consultant, 11/2020 - 4/2021)\n  + CitiGroup (Multiple roles, 2010-2020)\n  + Other companies: J.P. Morgan Chase Inc., American Express, Inc., Data Management and Marketing Inc.\n* **Tools and Core Skills:**\n  + Programming languages: Python, R, SAS, C, C++, Visual Basic Applications\n  + Machine Learning (ML) and AI\n  + Data analysis and modeling tools: Hadoop, Hive Sql, PySpark, SAS Enterprise Miner, SAS ETS, SAS Risk Dimensions\n  + Statistical analysis and modeling\n* **Notable Accomplishments/Achievements:**\n  + Developed credit risk models for acquisitions and existing customers management\n  + Conducted sales practice outlier scenario analysis and detection data mining\n  + Performed text data mining and sentiment analysis on customer call transcripts\n  + Built predictive models for credit default, fraud detection, and customer behavior\n* **Relevant Responsibilities and Projects:**\n  + Credit risk modeling and analysis\n  + Predictive modeling and data analysis for credit card high-risk account management\n  + Call center survey data analysis and text mining - brand sentiment analysis\n  + Sales practice outlier detection and analysis\n* **Gaps/Job Hopping:** \n  + Gap between 4/2021 and current date (not specified)\n  + Job hopping: Multiple job changes within a short span (e.g., between 2010-2021, changed jobs 4 times)\n\nThe candidate has a strong background in Python and ML, with experience in various industries, including finance and consulting. They have worked on multiple projects involving predictive modeling, data analysis, and risk management.\n\n\n--- /Volumes/workspace/default/default_resume/Kayuri Shah.txt ---\nRelevance: 0.56\nHere's a summary of Kayuri Shah's resume based on Python and ML skills:\n\n* **Experience**: 4+ years of experience in data science across companies like Mastercard and Mu Sigma.\n* **Tools and Core Skills**: \n  + Programming languages: Python, R\n  + Machine Learning: Linear regression, Logistic regression, K-means Clustering, Random Forest, Decision Tree, XGBoost\n  + Tools: Google Bigquery, Microsoft Azure, GCP, AWS, Amazon Redshift, S3\n* **Notable Achievements**:\n  + Built propensity models using XGBoost and Random Forest, reducing credit card misuse by 30% and increasing profitability by 15%.\n  + Derived CLV for over 100 million customers using XGBoost models, achieving a 17% revenue lift in 3 months.\n  + Developed customer persona segmentation models using K-Means Clustering.\n  + Implemented demand forecasting model, reducing order cycle time by 75%.\n* **Relevant Responsibilities and Projects**:\n  + Propensity modeling and customer segmentation.\n  + Time series forecasting and demand forecasting.\n  + Developed decision tree algorithm to identify credit card misuse.\n  + Built customer project mapping framework and product attachment analysis, generating $12M through real-time personalized recommendations.\n* **No gaps >6 months or job hopping observed**.\n\n\n--- /Volumes/workspace/default/default_resume/Raja_Agarwal.txt ---\nRelevance: 0.78\nHere's a summary of the candidate profile based on Python and ML skills:\n\n* **Companies and Experience**: Tesco (4 years 2 months), Mu Sigma Inc. (1 year 7 months); Total experience: 5+ years in data science and analytics for Fortune 100 retailers.\n* **Tools and Core Skills**: Python, SQL, Data Visualization (Tableau), AI/ML, Predictive and Prescriptive analysis, Problem-solving, Critical thinking.\n* **Notable Achievements**:\n  • Engineered an in-house Generative AI assistant to streamline data accessibility and analysis, replacing ~50 existing dashboards.\n  • Improved forecast accuracy from 92% to 98% using advanced statistical methods and machine learning algorithms.\n  • Optimized pricing strategy, resulting in a £1.2 million profit increase.\n  • Mitigated ~£25 million annual losses by identifying and de-ranging 100 underperforming products.\n* **Relevant Responsibilities and Projects**:\n  • Led a team to develop a Generative AI (Llama3 open source) powered news app.\n  • Developed a comprehensive business health dashboard using advanced data visualization techniques and sophisticated analytics.\n  • Collaborated with Strategy Management to develop and implement enhanced frameworks for Forecasting, Pricing, and Product Ranging.\n  • Conceptualized and built an algorithm to optimize pricing strategy for top-selling products.\n* **No gaps >6 months or job hopping observed**.\n\n\n--- /Volumes/workspace/default/default_resume/Sakshi_Gundawar.txt ---\nRelevance: 0.69\nHere's a summary of the candidate profile based on Python and ML skills:\n\n* **Experience and Domain Familiarity:**\n  * 6+ years of experience in data analysis across healthcare, telecom, and e-commerce sectors\n  * Worked with companies like Dassault Systemes, Tech Mahindra, and Medidata\n\n* **Tools and Core Skills:**\n  * Proficient in Python, R, and Java\n  * Skilled in ML libraries like TensorFlow, Scikit-learn, and PySpark\n  * Experienced in data preprocessing, modeling, and visualization using Pandas, NumPy, and Matplotlib\n  * Familiarity with cloud technologies like Google Cloud and AWS\n\n* **Notable Achievements and Measurable Impact:**\n  * Improved clinical AI prediction accuracy by implementing a robust data cleaning framework\n  * Increased CTR by 20% by optimizing predictive bidding models using statistical techniques\n  * Drove patient retention by 30% by applying Kaplan-Meier survival and logistic regression to 10M+ patient records\n  * Enhanced data processing speed by 40% for real-time search updates by optimizing MySQL queries and scaling Apache Spark data pipelines\n\n* **Relevant Responsibilities and Projects:**\n  * Engineered optimization for predictive bidding models using Python, Pandas, and SQL analysis\n  * Built customer segmentation based on engagement, demographics, and spending patterns using Python and Tableau\n  * Conducted a stock price forecasting study using ARIMA, Exponential Smoothing, and moving average\n  * Developed a robust Python and SQL validation pipeline, applying outlier detection to improve data quality by 99%\n\n* **No job hopping or gaps >6 months identified.**\n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    resumes = [\n",
    "        \"/Volumes/workspace/default/default_resume/Haiming Wang.txt\",\n",
    "        \"/Volumes/workspace/default/default_resume/Kayuri Shah.txt\",\n",
    "        \"/Volumes/workspace/default/default_resume/Raja_Agarwal.txt\",\n",
    "        \"/Volumes/workspace/default/default_resume/Sakshi_Gundawar.txt\",\n",
    "    ]\n",
    "\n",
    "    query = \"Summarize resume based on Python and ML skills .\"\n",
    "    summarize_all_resumes(resumes, query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fdb1f8-3734-4a1b-8f12-72c8e550dee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Future Work: \n",
    "\n",
    "1. Governance and Privacvy - PII redaction: Masking phone/email/address before retrieval is crucial for securing sensitive employee information.\n",
    "2. Creating a Front End UI for Hiring Managers which will integrate with ATS and have options for them to write query, see summaries and download only relevant resumes. \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Resume_Summarizer_ Agent_Pipeline_RAG +LLaMa + Transformers ",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
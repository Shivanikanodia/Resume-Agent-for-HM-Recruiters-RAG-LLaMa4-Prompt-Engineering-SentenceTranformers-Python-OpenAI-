{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf9e0a44-54b6-4010-89fd-8e3c3a08c5b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Resume Summarization AI Agent Pipeline (RAG + LLM): \n",
    "An end-to-end AI pipeline that leverages Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) on Databricks to generate professional summaries of resumes.\n",
    "\n",
    "#### Technologies Used:\n",
    " - Sentence Transformers (all-MiniLM-L6-v2) for semantic similarity and intelligent chunk retrieval.\n",
    "\n",
    " - Databricks-hosted LLaMA for scalable and secure LLM inference\n",
    "\n",
    " - Prompt Engineering to guide the model for clear, high-quality responses and to craft effective instructions and improve the LLM’s output relevance and clarity.\n",
    "\n",
    " - Delta live tables to access stored resumes. \n",
    "\n",
    " - REST API calls via requests - To communicate with the Databricks LLM endpoint for inference  \n",
    "\n",
    "\n",
    "#### Problem Statement:\n",
    "Recruiters and hiring managers often face the challenge of reviewing hundreds of resumes manually — a process that is both time-consuming and tedious.\n",
    "\n",
    "#### Project Goal:\n",
    "To streamline the recruitment process by generating concise and professional resume summaries using an LLM. The pipeline enhances accuracy and relevance by retrieving only the most semantically important content before passing it to the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3b7a798-a881-434f-8cbf-fb3d9e312f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Tools & Technologies:\n",
    "\n",
    "This project is built using the following key components:\n",
    "\n",
    "- Python – Core language for building and orchestrating the pipeline\n",
    "\n",
    "- Sentence Transformers (all-MiniLM-L6-v2) – For semantic search and similarity-based retrieval of relevant resume sections\n",
    "\n",
    "- Databricks LLM Endpoint (LLaMA 4) – Hosted large language model for generating high-quality resume summaries\n",
    "\n",
    "- Apache Spark – To efficiently read and process resumes stored in Unity Catalog Volumes\n",
    "\n",
    "- Prompt Engineering – To craft effective instructions and improve the LLM’s output relevance and clarity\n",
    "\n",
    "- REST API Calls (via requests) – To communicate with the Databricks LLM endpoint for inference  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e194dde-26bc-4595-93ea-304433ba183c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (5.1.2)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from sentence-transformers) (4.57.1)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from sentence-transformers) (2.9.0)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.11/site-packages (from sentence-transformers) (1.11.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: Pillow in /databricks/python3/lib/python3.11/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /databricks/python3/lib/python3.11/site-packages (from sentence-transformers) (4.10.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.23.5)\nRequirement already satisfied: regex!=2019.12.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.23)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6a9b55ed-ccb5-4949-859d-2fcb05702a7a/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2023.7.22)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# resume_summary_pipeline.py\n",
    "\n",
    "%pip install sentence-transformers\n",
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74659dd-be47-4831-8a98-dd45d1f9d4fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Reading and Chunking Resume: \n",
    "\n",
    "LLMs perform better with concise inputs. We are chunking long resumes into small parts (~800 chars) to be later searched semantically using **read_chunk_resume** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2b685c-3f53-470a-a548-34472330d061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_and_chunk_resume(path, width=800):\n",
    "    text = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return textwrap.wrap(text, width=width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "233435e8-74db-4872-bd5f-317876d31e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Path(path).read_text(),** Reads the content of the file at the specified path. \n",
    "\n",
    "**textwrap.wrap(text, width=800),** Splits the long resume string into chunks of **~800 characters** and avoids cutting off, which improves token management for LLMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0568c086-c88f-4646-91c9-4b97a5cc5239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Generating Embeddings:\n",
    "Embeddings converts chunks into high-dimensional vectors so we can find semantically similar text based on a query.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cddcf01-3c50-4dce-9de7-2191b74292f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embeddings(chunks, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, normalize_embeddings=True)\n",
    "    return embeddings, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74a6925b-269e-4c87-a580-ac121f59844c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The SentenceTransformer('all-MiniLM-L6-v2') model converts text chunks into dense numeric vectors (embeddings) that represent their meaning. The model.encode() function takes a list of text chunks and creates a 384-dimensional embedding for each one. This process returns two things:\n",
    "\n",
    "Embeddings – a list of vector representations for each text chunk.\n",
    "\n",
    "The model – which can be reused later to embed new text or queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9485bc6f-8bca-4801-9750-3f1b84c8777f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rerank_with_cross_encoder(query, candidate_pairs, cross_encoder_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_k=3):\n",
    "    #candidate_pairs: list of (idx, chunk_text)\n",
    "    ce = CrossEncoder(cross_encoder_name)\n",
    "    pairs = [(query, txt) for _, txt in candidate_pairs]\n",
    "    scores = ce.predict(pairs)  # higher is better\n",
    "    order = np.argsort(-scores)[:top_k]\n",
    "    return [candidate_pairs[i][0] for i in order]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b110daa-83d2-450c-9004-a846d81dd479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This function reranks text chunks based on their relevance to a query using a pretrained CrossEncoder model.\n",
    "It pairs the query with each text chunk, predicts relevance scores, sorts them in descending order, and returns the indices of the top_k most relevant chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d233b211-06ea-4a42-8f66-e12f89e3b741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_top_k_chunks(query, chunks, embeddings, model, top_k=5, use_cross_encoder=True, bi_encoder_candidates=10):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = cosine_similarity([q_emb], embeddings)[0]\n",
    "    cand_ids = np.argsort(-sims)[:max(top_k, bi_encoder_candidates)]\n",
    "    candidates = [(i, chunks[i]) for i in cand_ids]\n",
    "\n",
    "    if use_cross_encoder:\n",
    "        top_ids = rerank_with_cross_encoder(query, candidates, top_k=top_k)\n",
    "    else:\n",
    "        top_ids = list(cand_ids[:top_k])\n",
    "\n",
    "    return [chunks[i] for i in top_ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9c17c14-59dd-4c3c-9909-f85c7997ab64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finds the top-k most relevant text chunks to a query by encoding it, computing cosine similarity with chunk embeddings, and returning the highest-scoring matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f1aac8-501d-42d3-9cdd-831c92eb7123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Read Resume from Unity Catalog via Spark: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32496153-c393-45fb-8648-9906d17f368c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_resume_from_unity(file_path):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    df = spark.read.text(file_path)\n",
    "    return \"\\n\".join(row.value for row in df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efbe4d06-392c-4eaf-a82f-031edf712a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# .env\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e26c0cfe-8003-453c-95a6-fca5b07fa9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Calling Databricks LLM endpoint LLAMA-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be43b20-6c5f-487c-91d3-d0ab387833c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def call_databricks_llm(prompt, model=\"databricks-llama-4-maverick\", max_tokens=500):\n",
    "    \n",
    "    os.environ[\"DATABRICKS_HOST\"] = DATABRICKS_HOST\n",
    "    os.environ[\"DATABRICKS_TOKEN\"] = DATABRICKS_TOKEN\n",
    "\n",
    "    api_url = f\"https://dbc-7412fe25-6807.cloud.databricks.com/serving-endpoints/databricks-llama-4-maverick/invocations\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ['DATABRICKS_TOKEN']}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes resumes.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        print(\"\uD83D\uDD0D Response JSON:\", response_json)  # TEMP DEBUG LINE\n",
    "\n",
    "\n",
    "        # Try common response formats\n",
    "        if \"predictions\" in response_json:\n",
    "            return response_json[\"predictions\"][0][\"message\"][\"content\"]\n",
    "        elif \"choices\" in response_json:\n",
    "            return response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "        elif \"data\" in response_json:\n",
    "            return response_json[\"data\"][\"message\"]\n",
    "        else:\n",
    "            raise Exception(f\"❌ Unknown response structure: {response_json}\")\n",
    "    else:\n",
    "        raise Exception(f\"❌ Request failed with status code {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ecdef29-bcfe-4f86-bba5-d8187016d70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This script interacts with a Databricks-hosted large language model (LLM) by sending a prompt and receiving a summarized response. The API URL is constructed dynamically using the model name to ensure the request reaches the correct endpoint.\n",
    "\n",
    "The payload follows the OpenAI-style chat format, which includes a system message to set the assistant's behavior and a user message containing the actual prompt. \n",
    "\n",
    "The max_tokens parameter is used to limit the length of the model's response, ensuring it remains concise and controlled. Finally, the requests.post() function is used to make the HTTP request, sending the payload and headers to the LLM endpoint and retrieving the generated output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9fc4e1b-0b86-4b41-8452-60f52ceb60b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Format Prompt with Prompt Engineering: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189cd9f2-cf22-43d3-8e76-18a8448e69e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def format_prompt(context, query):\n",
    "    return f\"\"\"\n",
    "[System Role]\n",
    "You are a Resume Summarization Agent built to fully support hiring managers and recruiters by extracting key Skills, Projects and Experiences from their resumes. \n",
    "\n",
    "You understand:\n",
    "- Resume structure (Experience, Skills, Responsibilites, Education, Certifications)\n",
    "- Industry-specific terminologies.\n",
    "- Team Fitment and Cultural Fitment. \n",
    "\n",
    "Goal:\n",
    "Generate a concise, structured summary that enables rapid screening and reduces time spent reading full resumes.\n",
    "\n",
    "Resume:\n",
    "{context}\n",
    "\n",
    "Task:\n",
    "Summarize the candidate profile using these guidelines:\n",
    "\n",
    "1. Include Job titles, Companies worked  and total experience.\n",
    "2. Highlight technical tools and key skills. \n",
    "3. Mention notable accomplishments or quantifiable impact.\n",
    "4. Include relevant responsibilities, avoiding repetitive lines.\n",
    "5. Use clear bullet points for skimming efficiency.\n",
    "\n",
    "Constraints:\n",
    "- Avoid hallucination or extrapolation. \n",
    "- Do **not** add something which is not in resume. \n",
    "- Maintain a confident and neutral tone\n",
    "- Only summarize information explicitly present in the text.\n",
    "\n",
    "\uD83D\uDCE4 Output Format:\n",
    "- Bullet points only, no introduction or conclusion\n",
    "\n",
    "\uD83D\uDD0D Hiring Manager Query:\n",
    "Now respond to this specific question based on the resume above:\n",
    "**\"{query}\"**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61a424f4-f0c8-4741-aa75-5a446fcaff08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def relevance_score(summary, context_chunks):\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    summary_emb = model.encode([summary], normalize_embeddings=True)\n",
    "    context_emb = model.encode([\" \".join(context_chunks)], normalize_embeddings=True)\n",
    "    return float(cosine_similarity(summary_emb, context_emb)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc4ccd2-fd30-4a64-b9a2-8cdb88654025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def summarize_all_resumes(file_paths, query):\n",
    "    results = []\n",
    "    errors = 0\n",
    "    total_time = 0\n",
    "    relevance_scores = []\n",
    "\n",
    "    for path in file_paths:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            result = summarize_resume(path, query)\n",
    "            result_text = result if isinstance(result, str) else result[0]\n",
    "            relevance = relevance_score(result_text, read_and_chunk_resume(path))\n",
    "            relevance_scores.append(relevance)\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "        total_time += time.time() - start\n",
    "\n",
    "    print(\"\\n\uD83D\uDCCA Evaluation Summary:\")\n",
    "    print(f\"Average latency: {total_time/len(file_paths):.2f} sec/resume\")\n",
    "    print(f\"Error rate: {errors/len(file_paths)*100:.1f}%\")\n",
    "    print(f\"Average relevance score: {np.mean(relevance_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d6c19f2-5821-4366-b3bf-17473715aa0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def summarize_resume(file_path_txt, query):\n",
    "    chunks = read_and_chunk_resume(file_path_txt)\n",
    "    embeddings, model = get_embeddings(chunks)\n",
    "    top_chunks = get_top_k_chunks(query, chunks, embeddings, model)\n",
    "    context = \"\\n\\n\".join(top_chunks)\n",
    "    prompt = format_prompt(context, query)\n",
    "    summary = call_databricks_llm(prompt)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf02bb2-523f-4d71-9f0c-5f87d405b561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Response JSON: {'id': 'chatcmpl_13fbaee6-77ad-4775-b692-c002328e53b3', 'object': 'chat.completion', 'created': 1762110316, 'model': 'meta-llama-4-maverick-040225', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '* **Total Experience:** Over 14 years (from 5/2008 to 11/2017, and prior experience from 7/1996-8/2000)\\n* **Job Titles and Companies:**\\n  + Modeling Specialist, CitiGroup (11/2017 - onwards)\\n  + Business Analyst/Consultant, CitiGroup (8/2010 - 10/2011)\\n  + Statistical Risk Analyst/Consultant, Q-lytics Consulting at Bank of America and Toyota Financial (5/2008 - 7/2010)\\n  + Contract Consultant, Q-Lytics Consulting Inc. (7/1996-8/2000)\\n* **Technical Tools and Key Skills:**\\n  + Modeling and Analytical Tools: SAS, SAS Enterprise Miner, R, Revolution R, Python, AI/ML\\n  + Skills: Credit Risk modeling, Predictive Modeling, Data Analysis, Statistical analysis, Segmentation, Fraud detection, Model validation, Risk analytics\\n* **Notable Accomplishments/Responsibilities:**\\n  + Developed risk models for acquisitions and existing customers\\n  + Conducted predictive modeling and data analysis for credit default prevention programs\\n  + Performed model validations and analyses for regulatory and model governance\\n  + Built risk analytics project environment with SAS Risk Dimensions and SAS Credit Risk Management\\n* **Relevant Experience for Retail Work:**\\n  + Worked on credit card risk scoring and high-risk account management analytics at CitiGroup\\n  + Conducted data analysis and statistical analysis for credit risk policy at Bank of America\\n  + Experienced in lending product risk score modeling and portfolio risk analysis'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 912, 'completion_tokens': 329, 'total_tokens': 1241}}\n\uD83D\uDD0D Response JSON: {'id': 'chatcmpl_2e6b2377-6192-47a7-aba6-4f3e254236bc', 'object': 'chat.completion', 'created': 1762110321, 'model': 'meta-llama-4-maverick-040225', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '* Job Titles: Consultant, Decision Scientist, Data Scientist\\n* Companies Worked: Mastercard, Mu Sigma\\n* Total Experience: Approximately 4 years (Jan 2019 - Present)\\n* Technical Tools and Key Skills: \\n  + Programming: Python, R, SQL\\n  + Machine Learning: XGBoost, Random Forest, K-Means Clustering, Linear Regression, Logistic Regression\\n  + Data Analysis: Google Bigquery, Microsoft Azure, GCP, Adobe Analytics, Excel, Tableau, Power BI\\n  + Certifications: Microsoft Certified AI Engineer, Microsoft Certified Azure Data Scientist Associate\\n* Notable Accomplishments:\\n  + Achieved 17% revenue lift in 3 months for a Fortune 100 retailer through CLV-driven campaign\\n  + Reduced credit card misuse by 30% and increased profitability by 15% using decision tree algorithm\\n  + Increased conversions by 7.5% and boosted engagement by 40% through churn rate forecasting and targeted segmentation\\n  + Generated $12M through real-time personalized product recommendations\\n* Relevant Responsibilities:\\n  + Propensity modeling and customer segmentation\\n  + Demand forecasting and campaign analysis\\n  + Customer targeting and acquisition strategies\\n  + Predictive and prescriptive modeling for business optimization'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 1007, 'completion_tokens': 252, 'total_tokens': 1259}}\n\uD83D\uDD0D Response JSON: {'id': 'chatcmpl_3a043bfb-d2a2-48a0-84ed-c599d06d5b0b', 'object': 'chat.completion', 'created': 1762110328, 'model': 'meta-llama-4-maverick-040225', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '* Raja Agarwal, with 5+ years of experience, is a strong candidate for retail work.\\n* Job titles: Lead - Data Science Consultant, Senior Data Science Consultant, Data Science Consultant, Decision Scientist.\\n* Companies worked: Tesco, Mu Sigma Inc.\\n* Technical tools and key skills: MATLAB, Data Science, Analytics, Generative AI, Cross-Cultural Communication Skills, Supply Chain Management.\\n* Notable accomplishments:\\n  * Engineered an in-house Generative AI assistant, replacing ~50 existing dashboards.\\n  * Developed a data-based scenario model, predicting bottleneck stores for Tesco, resulting in ~£500K in additional sales.\\n  * Conceptualized and built an algorithm to optimize pricing strategy, resulting in a £1.2M profit increase.\\n  * Improved forecast accuracy from 92% to 98% and mitigated ~£25M annual losses by removing 100 underperforming products.\\n* Relevant responsibilities:\\n  * Led a team to design a comprehensive business health dashboard.\\n  * Collaborated with Strategy Management to develop enhanced frameworks for Forecasting, Pricing, and Product Ranging.\\n  * Coordinated directly with Deputy CFO and leadership to mitigate low online profitability.\\n  * Partnered with Supply Chain Management to design a dashboard, reducing YoY sell-out scenarios by 8%.'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 957, 'completion_tokens': 262, 'total_tokens': 1219}}\n\uD83D\uDD0D Response JSON: {'id': 'chatcmpl_ddd12431-1d4f-4410-af5f-c41289820042', 'object': 'chat.completion', 'created': 1762110336, 'model': 'meta-llama-4-maverick-040225', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '* Job Titles: Data Analyst \\n* Companies Worked: Dassault Systemes, Medidata, Tech Mahindra \\n* Total Experience: 6+ years \\n* Technical Tools and Key Skills: \\n  * Programming: Python, R, SQL, Py Spark \\n  * Data Visualization: Tableau, Power BI, Looker \\n  * Machine Learning and Statistics: Predictive Modeling, Time Series Forecasting, NLP \\n  * Databases: PostgreSQL, MySQL, MongoDB, Google Big Query \\n* Notable Accomplishments: \\n  * Drove patient retention by 30% through data analysis and trial design optimization \\n  * Reduced manual reporting by 60% with real-time dashboards \\n  * Improved data quality by 99% through outlier detection \\n  * Increased CTR by 20% through predictive bidding model optimization \\n  * Uplifted search result relevance by 17% through keyword targeting system refinement \\n* Relevant Responsibilities: \\n  * Data analysis and insights generation \\n  * Predictive modeling and process automation \\n  * Real-time dashboarding and data visualization \\n  * Data quality management and validation \\n* Relevant Projects: \\n  * Customer Churn Analysis & Insights \\n  * Time Series Forecasting for NVIDIA Stock \\n  * Keyword Targeting System \\n\\nThis candidate has relevant experience in e-commerce domain and has worked on projects related to customer retention, predictive modeling, and data analysis, making them a potential fit for retail work.'}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 1042, 'completion_tokens': 292, 'total_tokens': 1334}}\n\n\uD83D\uDCCA Evaluation Summary:\nAverage latency: 6.98 sec/resume\nError rate: 0.0%\nAverage relevance score: 0.74\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    resumes = [\n",
    "        \"/Volumes/workspace/default/default_resume/Haiming Wang.txt\",\n",
    "        \"/Volumes/workspace/default/default_resume/Kayuri Shah.txt\",\n",
    "        \"/Volumes/workspace/default/default_resume/Raja_Agarwal.txt\",\n",
    "        \"/Volumes/workspace/default/default_resume/Sakshi_Gundawar.txt\",\n",
    "    ]\n",
    "\n",
    "    query = \"Out of all provided candidate resumes, which one looks solid for retail work?\"\n",
    "    summarize_all_resumes(resumes, query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3e2144b-1fc6-412d-865e-e98e4b923b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**From RAG Pipeline, Summarry of resumes using consine similarity, Re-ranking, cross- encoder and Instructions have been generated. We can furhter fine tune it and check the quality of outputs.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fdb1f8-3734-4a1b-8f12-72c8e550dee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Future Work: \n",
    "\n",
    "1. Reranking: After vector search, I may pass top-20 chunks through a cross-encoder to boost precision of the final top-k.\n",
    "2. Governance and Privacvy - PII redaction: Masking phone/email/address before retrieval is crucial for securing sensitive employee information.\n",
    "3. Evaluation: Offline golden set + automatic faithfulness checks or using online feedback loop. \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Resume_Summarizer_ Agent_Pipeline_RAG +LLaMa + Transformers ",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

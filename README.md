# Resume Summarization AI Agent (RAG + LLM)

An end-to-end AI pipeline built on **Databricks** that leverages **Retrieval-Augmented Generation (RAG)** and **Large Language Models (LLMs)** to automatically generate **professional resume summaries**.

---

##  Problem Statement
Recruiters and hiring managers often have to manually review hundreds of resumes â€” a time-consuming and error-prone process.  
This project aims to **streamline recruitment** by automatically creating **concise, professional summaries** from resumes, ensuring only the **most relevant and important content** is highlighted.

---

##  Project Goals
- Reduce manual effort in resume screening.
- Generate **clear, concise, and relevant** summaries.
- Retrieve **only the most important sections** from resumes using semantic similarity before LLM processing.

---

##  Technologies Used
- **Python** â€“ Core language for building and orchestrating the pipeline.
- **Sentence Transformers** (`all-MiniLM-L6-v2`) â€“ For semantic search and intelligent chunk retrieval.
- **Databricks LLM Endpoint** (LLaMA 4) â€“ Hosted Large Language Model for high-quality summarization.
- **Apache Spark + Unity Catalog** â€“ Distributed processing and secure access to resume data.
- **Prompt Engineering** â€“ Optimized instructions for accurate and context-aware model output.
- **REST API Calls** â€“ Integration with Databricks LLM endpoints.
- **Databricks Notebooks** â€“ For pipeline execution and orchestration.

---

## âš™ï¸ How It Works
1. **Data Loading** â€“ Resumes are read from Unity Catalog Volumes using Apache Spark.
2. **Chunking & Embedding** â€“ Each resume is split into chunks, embedded using Sentence Transformers.
3. **Semantic Retrieval** â€“ Relevant chunks are retrieved based on query similarity.
4. **Prompt Construction** â€“ Retrieved content is passed into a carefully engineered LLM prompt.
5. **Summary Generation** â€“ LLaMA 4 (Databricks-hosted) generates the final professional summary.
6. **Output Storage** â€“ Summaries are saved back to Databricks for recruiter access.

---

## ğŸ—‚ Architecture Diagram (ASCII)

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Resume Dataset      â”‚
      â”‚ (Unity Catalog)     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Apache Spark Loader    â”‚
     â”‚ (Read & Preprocess)    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Sentence Transformers     â”‚
  â”‚ (Chunking + Embedding)    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Semantic Retrieval        â”‚
  â”‚ (Top Relevant Chunks)     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Prompt Engineering        â”‚
  â”‚ (LLM-ready context)       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Databricks LLaMA 4 LLM    â”‚
  â”‚ (Resume Summarization)    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Summary Storage           â”‚
  â”‚ (Databricks Output)       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## ğŸ“‚ Project Structure


---

## ğŸš€ Installation
Make sure you are running inside a **Databricks Notebook** with the following installed:
```bash
%pip install sentence-transformers



#### Usage:

Upload your resumes to a Unity Catalog Volume.

Open the notebook in Databricks.

#### Configure:

LLM endpoint URL & token

Resume data path in Unity Catalog

Run all cells â€” summaries will be generated and stored.

#### Benefits:

Cuts resume review time drastically.

Improves accuracy & relevance of summaries.

Can be scaled to thousands of resumes.


ğŸ“œ License

This project is intended for educational and internal enterprise use.
